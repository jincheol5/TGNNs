import os
import random
import threading
import queue
import wandb
import torch
import numpy as np
from typing_extensions import Literal
from tqdm import tqdm
from .data_utils import DataUtils
from .model_train_utils import ModelTrainUtils,EarlyStopping
from .metrics import Metrics

class ModelTrainer:
    @staticmethod
    def train(model,is_memory:bool=False,train_data_loader:list=None,val_data_loader:list=None,validate:bool=False,config:dict=None):
        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        optimizer=torch.optim.Adam(model.parameters(),lr=config['lr']) if config['optimizer']=='adam' else torch.optim.SGD(model.parameters(),lr=config['lr'])

        """
        Early stopping
        """
        if config['early_stop']:
            early_stop=EarlyStopping(patience=config['patience'])
        
        """
        model train
        """
        for epoch in tqdm(range(config['epochs']),desc=f"Training..."):
            model.train()
            loss_list=[]
            memory=None
            for batch in tqdm(train_data_loader,desc=f"Epoch {epoch+1}..."):
                if is_memory:
                    logit,memory=model(batch=batch,memory=memory,device=device)
                else:
                    logit=model(batch=batch,device=device)
                loss=Metrics.compute_TR_loss(logit=logit,label=batch['label'])
                loss_list.append(loss)

                # back propagation
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                if is_memory:
                    memory=memory.detach()

            """
            wandb log
            """
            epoch_loss=torch.stack(loss_list).mean().item()
            if config['wandb']:
                wandb.log({
                    f"loss":epoch_loss,
                },step=epoch)

            """
            validate
            """
            if validate:
                perform,memory=ModelTrainer.test(model=model,data_loader=val_data_loader)
                print(f"{epoch+1} epoch tR validation Acc: {perform['acc']} macro-f1: {perform['macrof1']} PR-AUC: {perform['prauc']} MCC: {perform['mcc']}")
            
            """
            Early stopping
            """
            if config['early_stop']:
                val_acc=perform['acc']
                pre_model=early_stop(val_acc=val_acc,model=model)
                if early_stop.early_stop:
                    model=pre_model
                    print(f"Early Stopping in epoch {epoch+1}")
                    break
        return memory

    @staticmethod
    def test(model,is_memory:bool=False,memory=None,data_loader=None):
        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        model.eval()
        
        """
        model test
        """
        logit_list=[]
        label_list=[]
        with torch.no_grad():
            for batch in tqdm(data_loader,desc=f"Evaluating..."):
                if is_memory:
                    logit,memory=model(batch=batch,memory=memory,device=device)
                else:
                    logit=model(batch=batch,device=device)
                logit_list.append(logit)
                label_list.append(batch['label'])
        
        perform={
            'acc':Metrics.compute_TR_acc(logit_list=logit_list,label_list=label_list),
            'macrof1':Metrics.compute_TR_macroF1(logit_list=logit_list,label_list=label_list),
            'prauc':Metrics.compute_TR_PRAUC(logit_list=logit_list,label_list=label_list),
            'mcc':Metrics.compute_TR_MCC(logit_list=logit_list,label_list=label_list)
        }
        return perform,memory